1.分词，英文的分词比较简单，直接用正则表达式匹配。中文可以调用jieba分词库。
```java
 public List<String> tokenize(String context) {
        List<String> words = new ArrayList<String>();
        //去除邮箱，url等；
        String code = context.replaceAll("(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]", "");
        code = code.replaceAll("\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*", "");

        code = code.replaceAll("import(\\sstatic)?\\s[\\S]+", "");

        code = code.replaceAll("package\\s[\\S]+", "");
        
        Pattern pattern = Pattern.compile("[A-Za-z_]+[A-Za-z0-9_]*");
        Matcher matcher = pattern.matcher(code);

        while (matcher.find()) {
            String token = matcher.group(); // 结果

            words.add(token);
        }
        return words;
    }



public List<String> chineseSegmenter(String sentence) {
        JiebaSegmenter segmenter = new JiebaSegmenter();
        List<SegToken> tokens = segmenter.process(sentence, JiebaSegmenter.SegMode.INDEX);
        List<String> ret = new ArrayList<String>();
        for (SegToken token : tokens) {
            ret.add(token.word);
        }
        return ret;
    }

```

2.去停用词，停用词表示经常出现对主题没有贡献的单词，如 is，are，then，... ... 如果对代码文件处理，停用词也可以包括代码中的关键字保留字，以及一些常用的库，比如集合等等。通常需要维护一个停用词表，遍历整篇文档一个一个地去除文档中的每一个停用词。

3.一些低频词，比如在整个语料库只出现一次两次，也可以考虑去除，降低数据的稀疏性。

4.代码的标识符通常是驼峰命名规则或者下划线命名规则，可以拆分成单独的单词。特殊情况：全是大写字母不是驼峰标识符，UUID；驼峰标识符中可能出现全大写的专有名词，UserDAORepository;一个标识符可能混用驼峰，下划线，以及全大写的专有名词；....
```
 public boolean isUnderlineCase(String token) {
        return token.contains("_");
    }

    public boolean isCamelCase(String token) {

        if (token.toUpperCase().equals(token)) return false;

        boolean flag = false;
        for (int i = 1; i < token.length() - 1; i++) {
            char c = token.charAt(i);

            if ((c - 'A') >= 0 && (c - 'Z') <= 0) {
                flag = true;
            }
        }
        return flag;
    }


    public List<String> splitSEWord(String word) {
        if (isCamelCase(word)) {
            int firstDownCase = 1;
            int followingUpperCase;
            for (; firstDownCase < word.length(); firstDownCase++) {
                char c = word.charAt(firstDownCase);
                if ((c - 'a') >= 0 && (c - 'z') <= 0) {
                    break;
                }
            }

            for (followingUpperCase = firstDownCase; followingUpperCase < word.length(); followingUpperCase++) {
                char c = word.charAt(followingUpperCase);
                if ((c - 'A') >= 0 && (c - 'Z') <= 0) {
                    break;
                }
            }

            if (firstDownCase == 1) {
                List list1 = splitSEWord(word.substring(0, followingUpperCase));
                List list2 = splitSEWord(word.substring(followingUpperCase));
                list1.addAll(list2);
                return list1;
            } else {
                List list1 = splitSEWord(word.substring(0, firstDownCase - 1));
                List list2 = splitSEWord(word.substring(firstDownCase - 1));
                list1.addAll(list2);
                return list1;
            }

        } else if (isUnderlineCase(word)) {
            List list = new ArrayList();

            for (String token : word.split("_")) {
                list.addAll(splitSEWord(token));
            }
            return list;
        } else {
            List list = new ArrayList<String>();
            list.add(word);
            return list;
        }

    }
```

5.字母全部小写化，比如Http ，http,HTTP等应该是同一个单词。

6. 词干提取，比如 swim，swimming，在主题建模时可以认为是同一个词。可以采用snowball 算法，提取词干。
```
import org.tartarus.snowball.ext.englishStemmer;
public String stemming(String word) {

        englishStemmer stemmer = new englishStemmer();
        stemmer.setCurrent(word);

        if (stemmer.stem()) {
            return stemmer.getCurrent();
        }
        return word;
    }
```

7.一些不规范的命名，可以选择去除，或者换成其对应的类名（有论文选择这样的方法处理java文档，但是其他语言存在类型推断的规则，甚至没有面向对象的支持，比较难做。）












